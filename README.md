# Deep-AutoQC
Image classification for the ENIGMA HALFpipe project at Charité  using deep learning - Bachelor Thesis
DeepAutoQC aims to assist researchers in classifying different type of report images including skull strip reports as either usable or unusable.

# Table of contents
1. [Introduction](#introduction)
    1. [Folder Structure](#folder-structure)
    2. [Scripts](#scripts)
        1. [Usage](#using-scripts)
        2. [Output](#output)
3. [Training Process](#training-process)
    1. [Argument Parsing](#argparse)
    2. [Data Preparation](#data-prep)
    3. [Model Training](#model-training)
    4. [Testing](#testing)
    5. [Running the Training Script](#running-the-training-script)


# Introduction <a name="introduction"></a>

Welcome to this project. It started out as the basis for a bachelor's thesis and has since then been continuously evolving. The original, stable version used for the thesis can be found in the `archive` branch.

The current focus is to improve the existing work. We're in the process of transitioning the code to use the PyTorch Lightning framework. Please note that this project is currently a work in progress.

## Folder Structure <a name="folder_structure"></a>
````
├── LICENSE.md                  <- License information.
├── README.md                   <- Overview of the project.
├── mypy.ini                    <- Configuration file for MyPy, a static type checker for Python.
├── pyproject.toml              <- Contains metadata about the project; used to manage project dependencies and versions.
├── requirements.in             <- Lists the top-level Python dependencies required to run the project.
├── requirements.txt            <- Lists all Python dependencies required to run the project (including nested dependencies).
└── src                         <- Source directory.
    └── deepautoqc              <- Main package containing the project's modules, scripts, and related files.
        ├── __init__.py
        ├── data                <- Directory for data related files.
        │   ├── test_data       <- Contains dummy test data files for the project.
        │   └── training_data   <- Contains dummy training data files for the project.
        ├── data_structures.py  <- Defines data structures used throughout the project.
        ├── lightning_module.py <- Defines the PyTorch Lightning module for the project and currently the training pipeline.
        ├── models.py           <- Contains neural network models definitions for Transfusion model and ResNet.
        ├── scripts             <- Contains utility scripts.
        │   ├── __init__.py
        │   ├── script_utils.py <- Contains utility functions used in scripts.
        │   ├── skull-strip_parser.py <- Used for parsing SVG files of skull-strip reports.
        │   └── tsnr_report_parser.py <- Used for parsing SVG files of tsnr reports.
        ├── utils.py            <- Contains utility functions used across the project.
└── tests                       <- Directory containing test scripts or test data.

````

## Using skull-strip_parser.py and tsnr_parser.py <a name="scripts"></a>

The `skull-strip_parser.py` and `tsnr_parser.py` scripts from `src/deepautoqc/scripts` are designed to process and extract information from SVG reports generated by neuroimaging software. The `skull-strip_parser.py` script works with **skull stripping** reports, while `tsnr_parser.py` works with **Temporal Signal-to-Noise Ratio (TSNR)** reports.

These scripts extract (and processes) image data and associated metadata from the SVG reports, save the images to a specified directory, and store the metadata in a pickle file for further use. In addition, the scripts allow users to review the images and label them as usable or unusable based on their quality.

### How to use <a name="using_scripts"></a>

After installing the package using `pip install .` from the main directory, the scripts can be accessed from the command line.

Use the `skullstrip_parser` and `tsnr_parser` commands with the `-d` flag specifying the directory containing the SVG reports and the `-s` flag specifying the directory where the processed images and pickle files will be saved.

For example:

```
skullstrip_parser.py -d /path/to/reports -s /path/to/save/directory
tsnr_parser.py -d /path/to/reports -s /path/to/save/directory
```

If you want to review the images manually and label them as usable or unusable, add the `-u` flag:

```
skullstrip_parser.py -d /path/to/reports -s /path/to/save/directory -u
tsnr_parser.py -d /path/to/reports -s /path/to/save/directory -u
```

### Output <a name="output"></a>

The scripts will save the images in the specified save directory and the metadata in a pickle file named based on the type of the report, the subject ID, and the dataset.

For example, for a skull stripping report for *subject 123456* from dataset *ds-toronto*, the scripts would create `ds-toronto_sub-123456_report-skull.pkl` and for a TSNR report for the same subject, they would create `ds-toronto_sub-123456_task-xyz_report-tsnr.pkl` only adding the subject's task.

These pickle files contain a list of namedtuples, where each namedtuple corresponds to one slice (e.g. axis y, column 5) in the report. The namedtuple includes the following fields:

- `id`: a unique identifier for the current slice
- `img`: the image data of this slice
- `label`: a string, either `usable` or `unusable`, indicating the quality of the slice

# Training Process <a name="training_process"></a>

This script is designed for training the MRI Quality Control (MRIAutoQC) model on brain scan data. The main parts of this process are argument parsing, data preparation, model training, and testing.

## Argument Parsing <a name="argparse"></a>

The script accepts several arguments that allow you to customize the training process:

- `-dl` or `--data_location`: Choose between `local` and `cluster` to determine the data paths. This determines whether the script uses data stored locally or on a cluster.
- `-mn` or `--model_name`: Selects the architecture of the Transfusion model used for training. Options include `small`, `tiny`, `wide`, and `tall`.
- `-e` or `--epochs`: The number of epochs to train the network. The default is 20.

## Data Preparation <a name="data_prep"></a>

The data for the model is prepared using the `BrainScanDataModule`. This PyTorch Lightning DataModule is responsible for loading the brain scan data from either a local path or a cluster, and preparing it for training.

## Model Training <a name="model_training"></a>

The `MRIAutoQC` model is trained using the PyTorch Lightning Trainer. The trainer is set up with several options:

- `accelerator="auto"`: Automatically selects the appropriate hardware accelerator (CPU, GPU, or TPU) available on the machine.
- `deterministic="warn"`: If any operations are performed that could cause non-deterministic behavior, a warning will be raised.
- `enable_progress_bar=True`: Enables a progress bar to be displayed during training.
- `max_epochs=args.epochs`: Sets the maximum number of epochs for training.

The trainer is then used to fit the model on the training and validation data, which are obtained from the DataModule.

## Testing <a name="testing"></a>

After training, the model is tested on the test data, which is again obtained from the DataModule.

## Running the Training Script <a name="run_train_script"></a>

To run the script, you can use a command similar to the following:

```
python train.py --data_location local --model_name small --epochs 30
```

This command would train the `small` model on `local` data for `30` epochs. Adjust the arguments as needed for your specific use case.

~~## Resume Training~~
~~You can resume training from a previously saved checkpoint by:~~
~~`python train.py -r ckpt/path`~~

~~## For Predictions~~
~~* check the `ckpts/` folder and set `MODEL_CKPT` to your desired `*model.pt` which will be loaded for predictions~~

~~Then, run `python3 test.py [-h] -i INPUT`. Remember `the following arguments are required: -i/--input` should contain the path to your svg file.~~
~~Check `predictions` folder for your output.~~

~~## Weights~~
~~* Weights of every trained model are stored in `/src/deepautoqc/weights` with their respective names and the dataset on which they got trained on~~
