{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CheckpointedModel(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (12): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): ReLU()\n",
       "  )\n",
       "  (global_pool): Sequential(\n",
       "    (0): AdaptiveMaxPool2d(output_size=(1, 1))\n",
       "    (1): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (prediction): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from pytorch_lightning import LightningModule\n",
    "from deepautoqc.models import TransfusionCBRCNN\n",
    "\n",
    "class CheckpointedModel(TransfusionCBRCNN, LightningModule):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "checkpoint_path = '/Users/Dominik/Charite/DeepAutoQC/src/deepautoqc/ckpts/2-636.ckpt'\n",
    "checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "\n",
    "# Assuming the labels were ['usable', 'unusable']\n",
    "model = CheckpointedModel(model_name='small', labels=['usable', 'unusable'])\n",
    "# Remove 'model.' prefix in the checkpoint keys\n",
    "state_dict = {k.replace('model.', ''): v for k, v in checkpoint['state_dict'].items()}\n",
    "\n",
    "# Load the modified state_dict into the model\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "#model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()  # set the model in evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReLU()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_layer = model.features[14]\n",
    "target_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cannot register a hook on a tensor that doesn't require gradient",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Set your CAM extractor\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchcam\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmethods\u001b[39;00m \u001b[39mimport\u001b[39;00m GradCAM,CAM\n\u001b[0;32m----> 3\u001b[0m cam_extractor \u001b[39m=\u001b[39m CAM(model)\n",
      "File \u001b[0;32m~/miniconda3/envs/autoqc/lib/python3.11/site-packages/torchcam/methods/activation.py:62\u001b[0m, in \u001b[0;36mCAM.__init__\u001b[0;34m(self, model, target_layer, fc_layer, input_shape, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(target_layer, \u001b[39mlist\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(target_layer) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     60\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mbase CAM does not support multiple target layers\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 62\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(model, target_layer, input_shape, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     64\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fc_layer, \u001b[39mstr\u001b[39m):\n\u001b[1;32m     65\u001b[0m     fc_name \u001b[39m=\u001b[39m fc_layer\n",
      "File \u001b[0;32m~/miniconda3/envs/autoqc/lib/python3.11/site-packages/torchcam/methods/core.py:54\u001b[0m, in \u001b[0;36m_CAM.__init__\u001b[0;34m(self, model, target_layer, input_shape, enable_hooks)\u001b[0m\n\u001b[1;32m     48\u001b[0m     target_names \u001b[39m=\u001b[39m [\n\u001b[1;32m     49\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_resolve_layer_name(layer) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(layer, nn\u001b[39m.\u001b[39mModule) \u001b[39melse\u001b[39;00m layer\n\u001b[1;32m     50\u001b[0m         \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m target_layer\n\u001b[1;32m     51\u001b[0m     ]\n\u001b[1;32m     52\u001b[0m \u001b[39melif\u001b[39;00m target_layer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[39m# If the layer is not specified, try automatic resolution\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     target_name \u001b[39m=\u001b[39m locate_candidate_layer(model, input_shape)  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[39m# Warn the user of the choice\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(target_name, \u001b[39mstr\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/autoqc/lib/python3.11/site-packages/torchcam/methods/_utils.py:43\u001b[0m, in \u001b[0;36mlocate_candidate_layer\u001b[0;34m(mod, input_shape)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39m# forward empty\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 43\u001b[0m     _ \u001b[39m=\u001b[39m mod(torch\u001b[39m.\u001b[39;49mrand(\u001b[39m1\u001b[39;49m, \u001b[39m*\u001b[39;49minput_shape, device\u001b[39m=\u001b[39;49m\u001b[39mnext\u001b[39;49m(mod\u001b[39m.\u001b[39;49mparameters())\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mdevice))\n\u001b[1;32m     45\u001b[0m \u001b[39m# Remove all temporary hooks\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39mfor\u001b[39;00m handle \u001b[39min\u001b[39;00m hook_handles:\n",
      "File \u001b[0;32m~/miniconda3/envs/autoqc/lib/python3.11/site-packages/torch/nn/modules/module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1536\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1538\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1539\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1540\u001b[0m     \u001b[39mfor\u001b[39;00m hook_id, hook \u001b[39min\u001b[39;00m (\n\u001b[1;32m   1541\u001b[0m         \u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1542\u001b[0m         \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1543\u001b[0m     ):\n",
      "File \u001b[0;32m~/Charite/DeepAutoQC/src/deepautoqc/models.py:217\u001b[0m, in \u001b[0;36mTransfusionCBRCNN.forward\u001b[0;34m(self, x, features)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, features\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    215\u001b[0m     \u001b[39m# x shape: batch_size, channels, height, width\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures(x)\n\u001b[1;32m    218\u001b[0m     \u001b[39m# x shape: batch_size, out_channels, h2, w2\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[39mif\u001b[39;00m features:\n",
      "File \u001b[0;32m~/miniconda3/envs/autoqc/lib/python3.11/site-packages/torch/nn/modules/module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1536\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1538\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1539\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1540\u001b[0m     \u001b[39mfor\u001b[39;00m hook_id, hook \u001b[39min\u001b[39;00m (\n\u001b[1;32m   1541\u001b[0m         \u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1542\u001b[0m         \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1543\u001b[0m     ):\n",
      "File \u001b[0;32m~/miniconda3/envs/autoqc/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/autoqc/lib/python3.11/site-packages/torch/nn/modules/module.py:1547\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1545\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1546\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1547\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39;49m, args, result)\n\u001b[1;32m   1549\u001b[0m \u001b[39mif\u001b[39;00m hook_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1550\u001b[0m     result \u001b[39m=\u001b[39m hook_result\n",
      "File \u001b[0;32m~/miniconda3/envs/autoqc/lib/python3.11/site-packages/torchcam/methods/gradient.py:53\u001b[0m, in \u001b[0;36m_GradCAM._hook_g\u001b[0;34m(self, module, input, output, idx)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Gradient hook\"\"\"\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hooks_enabled:\n\u001b[0;32m---> 53\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_handles\u001b[39m.\u001b[39mappend(output\u001b[39m.\u001b[39;49mregister_hook(partial(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_store_grad, idx\u001b[39m=\u001b[39;49midx)))\n",
      "File \u001b[0;32m~/miniconda3/envs/autoqc/lib/python3.11/site-packages/torch/_tensor.py:527\u001b[0m, in \u001b[0;36mTensor.register_hook\u001b[0;34m(self, hook)\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[39m.\u001b[39mregister_hook, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, hook)\n\u001b[1;32m    526\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequires_grad:\n\u001b[0;32m--> 527\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    528\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcannot register a hook on a tensor that \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdoesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt require gradient\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    529\u001b[0m     )\n\u001b[1;32m    530\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    531\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39m=\u001b[39m OrderedDict()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cannot register a hook on a tensor that doesn't require gradient"
     ]
    }
   ],
   "source": [
    "# Set your CAM extractor\n",
    "from torchcam.methods import GradCAM,CAM\n",
    "cam_extractor = CAM(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(690, 551, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 690, 551])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torchvision.transforms import functional as F\n",
    "from deepautoqc.utils import load_from_pickle\n",
    "from deepautoqc.data_structures import BrainScan\n",
    "\n",
    "# Assuming test_img is a numpy array of shape (690, 551, 3)\n",
    "# Reshape to (3, 690, 551) and normalize to [0, 1] range\n",
    "#test_img = test_img.transpose((2, 0, 1)) / 255.0\n",
    "\n",
    "pkl_path = '/Volumes/PortableSSD/data/skullstrip_rpt_processed_usable/_sub-102008_report-skull.pkl'\n",
    "\n",
    "subjects_data = load_from_pickle(pkl_path)\n",
    "test_img = subjects_data[1].img\n",
    "print(test_img.shape)\n",
    "\n",
    "test_img = test_img.transpose((2, 0, 1))\n",
    "\n",
    "# Add an extra dimension to simulate the batch size of 1\n",
    "#test_img = np.expand_dims(test_img, axis=0)\n",
    "\n",
    "# Convert to tensor\n",
    "test_img_tensor = torch.tensor(test_img, dtype=torch.float32)\n",
    "test_img_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cannot register a hook on a tensor that doesn't require gradient",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchcam\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmethods\u001b[39;00m \u001b[39mimport\u001b[39;00m SmoothGradCAMpp\n\u001b[1;32m      6\u001b[0m \u001b[39m#model = resnet18(pretrained=True).eval()\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m# Get your input\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m#img = read_image(\"path/to/your/image.png\")\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m# Preprocess it for your chosen model\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m#input_tensor = normalize(resize(img, (224, 224)) / 255., [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[39mwith\u001b[39;00m SmoothGradCAMpp(model) \u001b[39mas\u001b[39;00m cam_extractor:\n\u001b[1;32m     13\u001b[0m   \u001b[39m# Preprocess your data and feed it to the model\u001b[39;00m\n\u001b[1;32m     14\u001b[0m   out \u001b[39m=\u001b[39m model(test_img_tensor\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m))\n\u001b[1;32m     15\u001b[0m   \u001b[39m# Retrieve the CAM by passing the class index and the model output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/autoqc/lib/python3.11/site-packages/torchcam/methods/gradient.py:241\u001b[0m, in \u001b[0;36mSmoothGradCAMpp.__init__\u001b[0;34m(self, model, target_layer, num_samples, std, input_shape, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    232\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    233\u001b[0m     model: nn\u001b[39m.\u001b[39mModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    239\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(model, target_layer, input_shape, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    242\u001b[0m     \u001b[39m# Model scores is not used by the extractor\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_score_used \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/autoqc/lib/python3.11/site-packages/torchcam/methods/gradient.py:35\u001b[0m, in \u001b[0;36m_GradCAM.__init__\u001b[0;34m(self, model, target_layer, input_shape, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     28\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     29\u001b[0m     model: nn\u001b[39m.\u001b[39mModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m     33\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(model, target_layer, input_shape, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     36\u001b[0m     \u001b[39m# Init hook\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_g: List[Tensor] \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_names)  \u001b[39m# type: ignore[list-item]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/autoqc/lib/python3.11/site-packages/torchcam/methods/core.py:54\u001b[0m, in \u001b[0;36m_CAM.__init__\u001b[0;34m(self, model, target_layer, input_shape, enable_hooks)\u001b[0m\n\u001b[1;32m     48\u001b[0m     target_names \u001b[39m=\u001b[39m [\n\u001b[1;32m     49\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_resolve_layer_name(layer) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(layer, nn\u001b[39m.\u001b[39mModule) \u001b[39melse\u001b[39;00m layer\n\u001b[1;32m     50\u001b[0m         \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m target_layer\n\u001b[1;32m     51\u001b[0m     ]\n\u001b[1;32m     52\u001b[0m \u001b[39melif\u001b[39;00m target_layer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[39m# If the layer is not specified, try automatic resolution\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     target_name \u001b[39m=\u001b[39m locate_candidate_layer(model, input_shape)  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[39m# Warn the user of the choice\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(target_name, \u001b[39mstr\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/autoqc/lib/python3.11/site-packages/torchcam/methods/_utils.py:43\u001b[0m, in \u001b[0;36mlocate_candidate_layer\u001b[0;34m(mod, input_shape)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39m# forward empty\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 43\u001b[0m     _ \u001b[39m=\u001b[39m mod(torch\u001b[39m.\u001b[39;49mrand(\u001b[39m1\u001b[39;49m, \u001b[39m*\u001b[39;49minput_shape, device\u001b[39m=\u001b[39;49m\u001b[39mnext\u001b[39;49m(mod\u001b[39m.\u001b[39;49mparameters())\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mdevice))\n\u001b[1;32m     45\u001b[0m \u001b[39m# Remove all temporary hooks\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39mfor\u001b[39;00m handle \u001b[39min\u001b[39;00m hook_handles:\n",
      "File \u001b[0;32m~/miniconda3/envs/autoqc/lib/python3.11/site-packages/torch/nn/modules/module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1536\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1538\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1539\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1540\u001b[0m     \u001b[39mfor\u001b[39;00m hook_id, hook \u001b[39min\u001b[39;00m (\n\u001b[1;32m   1541\u001b[0m         \u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1542\u001b[0m         \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1543\u001b[0m     ):\n",
      "File \u001b[0;32m~/Charite/DeepAutoQC/src/deepautoqc/models.py:217\u001b[0m, in \u001b[0;36mTransfusionCBRCNN.forward\u001b[0;34m(self, x, features)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, features\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    215\u001b[0m     \u001b[39m# x shape: batch_size, channels, height, width\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures(x)\n\u001b[1;32m    218\u001b[0m     \u001b[39m# x shape: batch_size, out_channels, h2, w2\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[39mif\u001b[39;00m features:\n",
      "File \u001b[0;32m~/miniconda3/envs/autoqc/lib/python3.11/site-packages/torch/nn/modules/module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1536\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1538\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1539\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1540\u001b[0m     \u001b[39mfor\u001b[39;00m hook_id, hook \u001b[39min\u001b[39;00m (\n\u001b[1;32m   1541\u001b[0m         \u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1542\u001b[0m         \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1543\u001b[0m     ):\n",
      "File \u001b[0;32m~/miniconda3/envs/autoqc/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/autoqc/lib/python3.11/site-packages/torch/nn/modules/module.py:1547\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1545\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1546\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1547\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39;49m, args, result)\n\u001b[1;32m   1549\u001b[0m \u001b[39mif\u001b[39;00m hook_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1550\u001b[0m     result \u001b[39m=\u001b[39m hook_result\n",
      "File \u001b[0;32m~/miniconda3/envs/autoqc/lib/python3.11/site-packages/torchcam/methods/gradient.py:53\u001b[0m, in \u001b[0;36m_GradCAM._hook_g\u001b[0;34m(self, module, input, output, idx)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Gradient hook\"\"\"\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hooks_enabled:\n\u001b[0;32m---> 53\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_handles\u001b[39m.\u001b[39mappend(output\u001b[39m.\u001b[39;49mregister_hook(partial(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_store_grad, idx\u001b[39m=\u001b[39;49midx)))\n",
      "File \u001b[0;32m~/miniconda3/envs/autoqc/lib/python3.11/site-packages/torch/_tensor.py:527\u001b[0m, in \u001b[0;36mTensor.register_hook\u001b[0;34m(self, hook)\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[39m.\u001b[39mregister_hook, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, hook)\n\u001b[1;32m    526\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequires_grad:\n\u001b[0;32m--> 527\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    528\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcannot register a hook on a tensor that \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdoesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt require gradient\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    529\u001b[0m     )\n\u001b[1;32m    530\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    531\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39m=\u001b[39m OrderedDict()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cannot register a hook on a tensor that doesn't require gradient"
     ]
    }
   ],
   "source": [
    "from torchvision.io.image import read_image\n",
    "from torchvision.transforms.functional import normalize, resize, to_pil_image\n",
    "from torchvision.models import resnet18\n",
    "from torchcam.methods import SmoothGradCAMpp\n",
    "\n",
    "#model = resnet18(pretrained=True).eval()\n",
    "# Get your input\n",
    "#img = read_image(\"path/to/your/image.png\")\n",
    "# Preprocess it for your chosen model\n",
    "#input_tensor = normalize(resize(img, (224, 224)) / 255., [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "with SmoothGradCAMpp(model) as cam_extractor:\n",
    "  # Preprocess your data and feed it to the model\n",
    "  out = model(test_img_tensor.unsqueeze(0))\n",
    "  # Retrieve the CAM by passing the class index and the model output\n",
    "  activation_map = cam_extractor(out.squeeze(0).argmax().item(), out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoqc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
