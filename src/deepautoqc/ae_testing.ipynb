{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepautoqc.ae_architecture import Autoencoder, Encoder, Decoder\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from deepautoqc.data_structures import load_from_pickle, BrainScan\n",
    "import numpy as np\n",
    "import torchio as tio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_tensor(img: np.ndarray) -> torch.Tensor:\n",
    "    transform = tio.CropOrPad((3, 704, 800))\n",
    "    img = img.transpose((2, 0, 1))\n",
    "    img = torch.from_numpy(img)\n",
    "    img = tio.ScalarImage(tensor=img[None])\n",
    "    img = transform(img)\n",
    "    img = img.data[0]\n",
    "\n",
    "    return img.float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_reconstruction(model, image):\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Make sure the image is on the right device and has the expected dimensions\n",
    "    print(image.shape)\n",
    "    image = image.to(model.device).unsqueeze(0) # Add batch dimension if needed\n",
    "    print(image.shape)\n",
    "    # Pass the image through the autoencoder\n",
    "    with torch.no_grad():\n",
    "        reconstructed_image = model(image)\n",
    "\n",
    "    # Convert to numpy and remove batch dimension\n",
    "    original_image_np = image.squeeze(0).cpu().numpy()\n",
    "    reconstructed_image_np = reconstructed_image.squeeze(0).cpu().numpy()\n",
    "\n",
    "    # Assuming the images are single-channel, you might need to use squeeze to remove the channel dimension\n",
    "    original_image_np = original_image_np.squeeze()\n",
    "    reconstructed_image_np = reconstructed_image_np.squeeze()\n",
    "\n",
    "    original_image_np = original_image_np.transpose((1, 2, 0))\n",
    "    reconstructed_image_np = reconstructed_image_np.transpose((1, 2, 0))\n",
    "\n",
    "    print(\"ORIG\", original_image_np.min(), original_image_np.max())\n",
    "    print(\"REC\",reconstructed_image_np.min(), reconstructed_image_np.max())\n",
    "\n",
    "    # Create a subplot to display the original and reconstructed images\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axes[0].imshow(original_image_np, cmap='gray')\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(reconstructed_image_np, cmap='gray')\n",
    "    axes[1].set_title('Reconstructed Image')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# image = load_an_image() # Some function to load an image tensor\n",
    "# display_reconstruction(model, image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_anomaly(model, image, threshold):\n",
    "    model.eval()\n",
    "    \n",
    "    image = image.to(model.device).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        reconstructed_image = model(image)\n",
    "\n",
    "    error = torch.nn.functional.mse_loss(image, reconstructed_image)\n",
    "\n",
    "    #classification = \"normal\" if error < threshold else \"anomaly\"\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 64\n",
    "#model = Autoencoder(base_channel_size=32, latent_dim=latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"/Users/Dominik/Charite/DeepAutoQC/src/deepautoqc/ckpts/AE_384-22-1886.ckpt\"\n",
    "ckpt_path_2 = \"/Users/Dominik/Charite/DeepAutoQC/src/deepautoqc/ckpts/AE_64-24-2050.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_from_checkpoint(ckpt_path_2, map_location=torch.device('cpu'))\n",
    "model = Autoencoder.load_from_checkpoint(ckpt_path, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (encoder): Encoder(\n",
       "    (net): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): GELU(approximate='none')\n",
       "      (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (5): GELU(approximate='none')\n",
       "      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): GELU(approximate='none')\n",
       "      (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (9): GELU(approximate='none')\n",
       "      (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): GELU(approximate='none')\n",
       "      (12): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (13): GELU(approximate='none')\n",
       "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): GELU(approximate='none')\n",
       "      (16): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (17): GELU(approximate='none')\n",
       "      (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (19): GELU(approximate='none')\n",
       "      (20): Flatten(start_dim=1, end_dim=-1)\n",
       "      (21): Linear(in_features=281600, out_features=384, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (linear): Sequential(\n",
       "      (0): Linear(in_features=384, out_features=281600, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "    )\n",
       "    (net): Sequential(\n",
       "      (0): ConvTranspose2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): GELU(approximate='none')\n",
       "      (4): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (5): GELU(approximate='none')\n",
       "      (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): GELU(approximate='none')\n",
       "      (8): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (9): GELU(approximate='none')\n",
       "      (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): GELU(approximate='none')\n",
       "      (12): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (13): GELU(approximate='none')\n",
       "      (14): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): GELU(approximate='none')\n",
       "      (16): ConvTranspose2d(64, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (17): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_subject_data =  load_from_pickle(\"/Volumes/PortableSSD/data/skullstrip_rpt_processed_unusable/ds-hcp_sub-122620_skull_strip_report_ds-hcp_sub-122620_report-skull.pkl\")\n",
    "good_subject_data = load_from_pickle(\"/Volumes/PortableSSD/data/skullstrip_rpt_processed_usable/ds-pnc_chunk-9_reports_sub-607733289129_report-skull.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in bad_subject_data:\n",
    "    img = subject.img\n",
    "    img_tensor = load_to_tensor(img=img)\n",
    "\n",
    "    #display_reconstruction(model=model, image=img_tensor)\n",
    "    error = is_anomaly(model, image=img_tensor, threshold=0)\n",
    "    print(error.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in good_subject_data:\n",
    "    img = subject.img\n",
    "    img_tensor = load_to_tensor(img=img)\n",
    "\n",
    "    #display_reconstruction(model=model, image=img_tensor)\n",
    "    error = is_anomaly(model, image=img_tensor, threshold=0)\n",
    "    print(error.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in bad_subject_data:\n",
    "    img = subject.img\n",
    "    img_tensor = load_to_tensor(img=img)\n",
    "\n",
    "    display_reconstruction(model=model, image=img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in good_subject_data:\n",
    "    img = subject.img\n",
    "    img_tensor = load_to_tensor(img=img)\n",
    "\n",
    "    display_reconstruction(model=model, image=img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Dominik/miniconda3/envs/autoqc/lib/python3.11/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/Users/Dominik/miniconda3/envs/autoqc/lib/python3.11/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/Users/Dominik/miniconda3/envs/autoqc/lib/python3.11/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/Users/Dominik/miniconda3/envs/autoqc/lib/python3.11/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in bad_subject_data:\n",
    "    img = subject.img\n",
    "    img_tensor = load_to_tensor(img=img)\n",
    "\n",
    "    img_tensor = img_tensor.to(model.device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        encoder_output = model.encoder(img_tensor)\n",
    "        encoder_outputs.append(encoder_output.cpu().numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in good_subject_data:\n",
    "    img = subject.img\n",
    "    img_tensor = load_to_tensor(img=img)\n",
    "\n",
    "    img_tensor = img_tensor.to(model.device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        encoder_output = model.encoder(img_tensor)\n",
    "        encoder_outputs.append(encoder_output.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs = np.concatenate(encoder_outputs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    }
   ],
   "source": [
    "reducer = umap.UMAP()\n",
    "embedding = reducer.fit_transform(encoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the result\n",
    "plt.scatter(embedding[:, 0], embedding[:, 1])\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title('UMAP projection of the data', fontsize=24)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
